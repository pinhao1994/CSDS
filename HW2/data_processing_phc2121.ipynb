{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "***\n",
    "* Assigned: 02/07\n",
    "* Due: **02/20 at 11:59pm** electronically\n",
    "* This assignment is worth 100 points.\n",
    "\n",
    "**Please do read the instructions for each response carefully, since a whole lot of different variety of reponses are involved in the assignment (including pasting code snippets, writing executable code, small writeups). You don't wanna be losing points for silly errors.**\n",
    "\n",
    "### Jupyter Notes:\n",
    "\n",
    "* You **may** create new IPython notebook cells to use for e.g. testing, debugging, exploring, etc.- this is encouraged in fact!\n",
    "  * you can press shift+enter to execute the code in the cell that your cursor is in.\n",
    "* When you see `In [*]:` to the left of the cell you are executing, this means that the code / query is _running_. Please wait for the execution to complete\n",
    "    * **If the cell is hanging- i.e. running for too long: you can restart the kernel**\n",
    "    * To restart kernel using the menu bar: \"Kernel >> Restart >> Clear all outputs & restart\"), then re-execute cells from the top\n",
    "* _Have fun!_\n",
    "\n",
    "\n",
    "### Setup Your Credentials\n",
    "\n",
    "Update the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your columbia uni that is used in SSOL\n",
    "#\n",
    "# IMPORTANT:  make sure this is consistent with the uni/alias used as your @columbia.edu email in SSOL\n",
    "#\n",
    "UNI = \"phc2121\"\n",
    "\n",
    "# your instabase username (if you go to the instabase homepage, your username should be in the URL)\n",
    "USER = \"phc2121\"\n",
    "\n",
    "# your repository name containing \n",
    "REPO = \"CSDS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this lab, you will use various types of tools -- from high-level tools like Data Wrangler to command line tools like `sed` and `awk` -- to perform data parsing and extraction from data encoded into a text file.  The goal of this lab is simply to gain experience with these tools and compare and contrast their usage.\n",
    "\n",
    "The `lab` directory contains two datasets that you will work with:\n",
    "\n",
    "1. A dataset of all the movies in 2013 from January to March (`2013films.txt`). It contains Movie name, Production house, Genre, Publisher and some other details.\n",
    "\n",
    "2. The second dataset (`worldcup.txt`) is a snippet of the following Wikipedia webpage on [FIFA (Soccer) World Cup](http://en.wikipedia.org/wiki/FIFA_World_Cup).\n",
    "Specifically it is a partially cleaned-up wiki source for the table toward the end of the page that lists teams finishing in the top 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Get Wrangling !!\n",
    "***\n",
    "Download the [Trifacta Wrangler](https://www.trifacta.com/products/wrangler/) tool. Load both the datasets into wrangler and try playing around with the tool.\n",
    "\n",
    "Some tips using Wrangler:\n",
    "\n",
    "* Check out the introduction [video](https://vimeo.com/19185801) to get a feel of how wrangler works.\n",
    "* You may wanna start off by loading ~50 lines of data instead of the entire files and play around with the tool.\n",
    "* Wrangler responds to mouse highlights and clicks on the displayed table cells by suggesting operations on the left sidebar.  \n",
    "* Hovering over each element shows the result in the table view.  \n",
    "* Clicking adds the operation.  \n",
    "* Clear the sidebar by clicking the colored row above the schema row.\n",
    "\n",
    "## Tasks:\n",
    "\n",
    "Use Data Wrangler for the following two datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2013films.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., Movie name, Production/Distribtuion house, Director, Genre and publisher. \n",
    "\n",
    "        'A dark truth, Magnolia Pictures, Damian Lee, Action,ComingSoon.net\n",
    "         Table No. 21, Eros International, Aditya Datt, Thriller, BoxOfficeMojo.com\n",
    "         ...\n",
    "        \n",
    "For the purpose of explanation columns are separated by ||. You can choose any pattern to extract information. \n",
    "\n",
    "1. Movie name can be identified as first column in every line formatted as ''[[ <movie name> ]]''  \n",
    "1. Production/Distribution house is the following column that is formatted as [[< Production house>]]  \n",
    "1. Director name can be identified with \"(director)\" tag that follows the name  \n",
    "1. Genre is present in the next column but make sure to extract only second part that is separated by | operator. For eg. in [Action film|Action] relevant information is Action  \n",
    "1. Publisher name can be identified in the last column with format \"publisher=<publisher name>\"  \n",
    "1. It may help to skip first few lines that contains html code, so that you process actual records.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* Use wrangler to clean the data, then determine which Production/Distribution house produced maximum movies.\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler.\n",
    "Export the script and paste it in the cell below. Do not bother executing it here in the notebook.\n",
    "* Stanford also has an online open-source version of Trifacta Wrangler that can be used [here](http://vis.stanford.edu/wrangler/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below\n",
    "delete row: matches([column1], `{start}!`)\n",
    "delete row: matches([column1], `\\|-`)\n",
    "delete row: matches([column1], `{start}=`)\n",
    "delete row: matches([column1], `DO NOT ADD FILMS`)\n",
    "delete row: matches([column1], `class=|style=|rowspan=`)\n",
    "delete row: in(sourcerownumber(), [2, 183, 179, 359, 362, 537, 536, 660, 358])\n",
    "split col: column1 on: '||' limit: 1\n",
    "replace col: column2 with: '' on: `\\| \\'\\'[[` global: true\n",
    "replace col: column2 with: '' on: `]]\\'\\' ` global: true\n",
    "replace col: column2 with: '' on: `{any}+\\|`\n",
    "replace col: column2 with: '' on: `\\|\\'\\'[[` global: true\n",
    "rename col: column2 to: 'MovieName'\n",
    "split col: column3 on: '||' limit: 1\n",
    "replace col: column1 with: '' on: ` [[` global: true\n",
    "replace col: column1 with: '' on: `]] ` global: true\n",
    "replace col: column1 with: '' on: `{start}{delim}`\n",
    "replace col: column1 with: '' on: `{delim}{end}`\n",
    "rename col: column1 to: 'ProductionDistributionHouse'\n",
    "split col: column2 on: `\\(director`\n",
    "replace col: column1 with: '' on: ` [[` global: true\n",
    "replace col: column1 with: '' on: `]] ` global: true\n",
    "replace col: column1 with: '' on: `{start}{delim}`\n",
    "replace col: column1 with: '' on: `{delim}{end}`\n",
    "rename col: column1 to: 'Director'\n",
    "split col: column3 on: '||' limit: 1\n",
    "drop col: column2\n",
    "split col: column4 on: `\\|\\|`\n",
    "replace col: column1 with: '' on: ` [[` global: true\n",
    "replace col: column1 with: '' on: `]]` global: true\n",
    "replace col: column1 with: '' on: `{start}{delim}`\n",
    "replace col: column1 with: '' on: `{delim}{end}`\n",
    "split col: column1 on: `\\|`\n",
    "replace col: column4 with: '' on: ` film\\|{any}+` global: true\n",
    "set col: column4 value: ifmissing($col, column3)\n",
    "drop col: column3\n",
    "rename col: column4 to: 'Genre'\n",
    "set col: Genre value: ifmissing($col, 'N\\/A')\n",
    "extract col: column2 on: `publisher={any}+ \\||publisher={any}+\\|`\n",
    "drop col: column2\n",
    "replace col: column1 with: '' on: `\\|{any}+` global: true\n",
    "replace col: column1 with: '' on: `{delim}{end}`\n",
    "replace col: column1 with: '' on: `{delim}{end}`\n",
    "replace col: column1 with: '' on: `publisher=` global: true\n",
    "set col: column1 value: ifmissing($col, 'N\\/A')\n",
    "replace col: Director with: '' on: `]]{any}+| \\({any}+` global: true\n",
    "replace col: column1 with: '' on: `[[` global: true\n",
    "rename col: column1 to: 'Publisher'\n",
    "replace col: ProductionDistributionHouse with: '' on: `/{any}+|\\|{any}+|[[|\\({any}+`\n",
    "replace col: ProductionDistributionHouse with: '' on: `{delim}{end}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangler script to get the answer for the question\n",
    "aggregate value: count() group: ProductionDistributionHouse\n",
    "sort order: -row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Question) Which Production/Distribution house produced maximum movies?\n",
    "#### Warner Bros: 20\n",
    "#### 20th Century Fox: 14\n",
    "#### Universal Studios: 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worldcup.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., each line in the output contains a country, a year, and the position of the country in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "It may help to \n",
    "\n",
    "1. Skip the first 20 or so rows of table headers and other text, so that the data wrangler works with are \"record text\".  \n",
    "2. Delete the rows that are clearly HTML formatting content\n",
    "3. Extract the relevant data from the remaining column into new columns\n",
    "4. Use the fill operation\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* According to the dataset, how often has each country won the world cup?\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler. Export the script and paste it in the cell below. Do not bother executing it here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below\n",
    "delete row: matches([column1], `\\|-`)\n",
    "delete row: in(sourcerownumber(), [1,171,172,173])\n",
    "replace col: column1 with: '' on: `\\|align=center`\n",
    "replace col: column1 with: '' on: `\\|bgcolor=\\#FFF68F` global: true\n",
    "replace col: column1 with: '' on: `[[\\#1\\|\\*]]` global: true\n",
    "delete row: matches([column1], `{start}\\|{digit}+{end}`)\n",
    "extractlist col: column1 on: `\\|{digit}{4}` as: 'Years'\n",
    "extract col: column1 on: `fb\\|{upper}{3}` quote: ''\n",
    "replace col: column2 with: '' on: `fb\\|` global: true\n",
    "set col: column2 value: fill(column2) order: sourcerownumber()\n",
    "replace col: Years with: '' on: `\\|` global: true\n",
    "rename col: column2 to: 'Country'\n",
    "settype col: Years type: 'Array'\n",
    "derive value: (mod(rownumber(), 5)) - 1 group: Country order: sourcerownumber() as: 'Position'\n",
    "replace col: Position with: '4' on: `-1` global: true\n",
    "flatten col: Years\n",
    "drop col: column1\n",
    "delete row: ismissing([Years])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangler script to get the answer for the question\n",
    "aggregate value: countif(Position == 1) group: Country\n",
    "sort order: -countif\n",
    "rename col: countif to: 'Count_of_first_position'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Question) how often has each country won the world cup?\n",
    "#### BRA won the 1st position: 5 times\n",
    "#### ITA won the 1st position: 4 times\n",
    "#### GER won the 1st position: 3 times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Grep, Sed & Awk\n",
    "***\n",
    "\n",
    "The set of three UNIX tools, sed, awk, and grep, can be very useful for quickly cleaning up and transforming data for further analysis (and have been around since the inception of UNIX). In conjunction with other unix utilities like sort, uniq, tail, head, etc., you can accomplish many simple data parsing and cleaning tasks with these tools. You are encouraged to play with these tools and familiarize yourselves with the basic usage of these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "grep 'regexp' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently (using UNIX pipelining):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "cat filename | grep 'regexp'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains only those lines from the file that match the regular expression. Two options to grep are useful: grep -v will output those lines that do not match the regular expression, and grep -i will ignore case while matching. See the manual (man grep) (or online resources) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sed stands for stream editor. Basic syntax for sed is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "sed 's/regexp/replacement/g' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line in the intput, the portion of the line that matches regexp (if any) is replaced with replacement. Sed is quite powerful within the limits of operating on single line at a time. You can use \\( \\) to refer to parts of the pattern match. In the first sed command above, the sub-expression within \\( \\) extracts the user id, which is available to be used in the replacement as \\1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, awk is a powerful scripting language (not unlike perl). The basic syntax of awk is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "awk -F',' 'BEGIN{commands} /regexp1/ {command1} /regexp2/ {command2} END{commands}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line, the regular expressions are matched in order, and if there is a match, the corresponding command is executed (multiple commands may be executed for the same line). BEGIN and END are both optional. The -F',' specifies that the lines should be split into fields using the separator \",\", and those fields are available to the regular expressions and the commands as $1, $2, etc. See the manual (man awk) or online resources for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES\n",
    "\n",
    "#### Note\n",
    "There's nothing to submit in the examples section. Task to carry out and submit will follow after this section. Just play around and get a hang of the bash commands described here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "We start off by copying the files from our instabase repository to the VM filesystem our instabse instance is running on.\n",
    "Remember, you'll have to execute the cell below everytime the VM is restarted (happens when you close and restart the notebook) before you can proceed with the bash examples that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ib.open(\"labor.csv\") as f:\n",
    "    labor=f.read()\n",
    "with ib.open(\"crime-clean.txt\") as f:\n",
    "    crime_clean=f.read()\n",
    "with ib.open(\"crime-unclean.txt\") as f:\n",
    "    crime_unclean=f.read()\n",
    "\n",
    "open('/tmp/labor.csv','w').write(labor)\n",
    "open('/tmp/crime-clean.txt','w').write(crime_clean)\n",
    "open('/tmp/crime_unclean.txt','w').write(crime_unclean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples to give you a flavor of the tools and what one can do with them.\n",
    "\n",
    "* Perform the equivalent of wrap on labor.csv (i.e., merge consecutive groups of lines referring to the same record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/labor.csv | awk '/^Series Id:/ {print combined; combined = $0} \n",
    "                    !/^Series Id:/ {combined = combined\", \"$0;}\n",
    "                    END {print combined}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that all bash cells begin with **%%bash**. This indicates that, what follows is a bash code/script.*\n",
    "\n",
    "\n",
    "* On crime-clean.txt, the following command does a fill (first row of output: \"Alabama, 2004, 4029.3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/crime-clean.txt | grep -v '^,$' | awk '/^[A-Z]/ {state = $4} !/^[A-Z]/ {print state, $0}'\n",
    "#cat crime-clean.txt | grep -v '^,$' | awk '/^[A-Z]/ {state = $4} !/^[A-Z]/ {print state, $0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On crime-clean.txt, the following script cleans the data. The following works assuming perfectly homogenous data (as the example on the Wrangler website is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat /tmp/crime-clean.txt | grep -v '^,$' | sed 's/,$//g; s/Reported crime in //; s/[0-9]*,//' | awk -F',' 'BEGIN {printf \"State, 2004, 2005, 2006, 2007, 2008\"} /^[A-Z]/ {print c; c=$0} !/^[A-Z]/ {c=c\", \"$0;} END {print c;}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* On crime-unclean.txt the follow script perfroms the same cleaning as above, but allows incomplete information (e.g., some years may be missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat /tmp/crime_unclean.txt | grep -v '^,$' | sed 's/Reported crime in //;' | \n",
    "        awk -F',' 'BEGIN {printf \"State, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\\n\"} /^[A-Z]/ || /^$/ { if(state) { printf(state); for(i = 2000; i <= 2008; i++) { if(array[i]) {printf(\"%s,\", array[i])} else {printf(\"0,\")} }; printf(\"\\n\");} state=$0; delete array} !/^[A-Z]/ {array[$1] = $2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided the last example to show how powerful awk can be. However if you need to write a long command like this, you may be better off using a proper scripting language like perl or python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the awk \"split\" function and \"for loop\" constructs on World Cup data, to again generate output as follows, i.e., each line in the output contains a country, a year, and the position of the county in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "* Start with the given script that cleans up the data a little bit.\n",
    "* No need to re-answer the questions in the Wrangler section, but recompute them to ensure your answers are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ib.open(\"worldcup.txt\") as f:\n",
    "    world_cup=f.read()\n",
    "    \n",
    "open('/tmp/worldcup.txt','w').write(world_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\tYear\tPosition\n",
      "BRA\t1958\t1\n",
      "BRA\t1962\t1\n",
      "BRA\t1970\t1\n",
      "BRA\t1994\t1\n",
      "BRA\t2002\t1\n",
      "BRA\t1950\t2\n",
      "BRA\t1998\t2\n",
      "BRA\t1938\t3\n",
      "BRA\t1978\t3\n",
      "BRA\t1974\t4\n",
      "ITA\t1934\t1\n",
      "ITA\t1938\t1\n",
      "ITA\t1982\t1\n",
      "ITA\t2006\t1\n",
      "ITA\t1970\t2\n",
      "ITA\t1994\t2\n",
      "ITA\t1990\t3\n",
      "ITA\t1978\t4\n",
      "GER\t1954\t1\n",
      "GER\t1974\t1\n",
      "GER\t1990\t1\n",
      "GER\t1966\t2\n",
      "GER\t1982\t2\n",
      "GER\t1986\t2\n",
      "GER\t2002\t2\n",
      "GER\t1934\t3\n",
      "GER\t1970\t3\n",
      "GER\t2006\t3\n",
      "GER\t2010\t3\n",
      "GER\t1958\t4\n",
      "ARG\t1978\t1\n",
      "ARG\t1986\t1\n",
      "ARG\t1930\t2\n",
      "ARG\t1990\t2\n",
      "URU\t1930\t1\n",
      "URU\t1950\t1\n",
      "URU\t1954\t4\n",
      "URU\t1970\t4\n",
      "URU\t2010\t4\n",
      "FRA\t1998\t1\n",
      "FRA\t2006\t2\n",
      "FRA\t1958\t3\n",
      "FRA\t1986\t3\n",
      "FRA\t1982\t4\n",
      "ENG\t1966\t1\n",
      "ENG\t1990\t4\n",
      "ESP\t2010\t1\n",
      "ESP\t1950\t4\n",
      "NED\t1974\t2\n",
      "NED\t1978\t2\n",
      "NED\t2010\t2\n",
      "NED\t1998\t4\n",
      "TCH\t1934\t2\n",
      "TCH\t1962\t2\n",
      "HUN\t1938\t2\n",
      "HUN\t1954\t2\n",
      "SWE\t1958\t2\n",
      "SWE\t1950\t3\n",
      "SWE\t1994\t3\n",
      "SWE\t1938\t4\n",
      "POL\t1974\t3\n",
      "POL\t1982\t3\n",
      "AUT\t1954\t3\n",
      "AUT\t1934\t4\n",
      "POR\t1966\t3\n",
      "POR\t2006\t4\n",
      "USA\t1930\t3\n",
      "CHI\t1962\t3\n",
      "CRO\t1998\t3\n",
      "TUR\t2002\t3\n",
      "YUG\t1930\t4\n",
      "YUG\t1962\t4\n",
      "URS\t1966\t4\n",
      "BEL\t1986\t4\n",
      "BUL\t1994\t4\n",
      "KOR\t2002\t4\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/worldcup.txt | sed 's/\\[\\[\\([0-9]*\\)[^]]*\\]\\]/\\1/g; s/.*fb|\\([A-Za-z]*\\)}}/\\1/g; s/<sup><\\/sup>//g; s/|bgcolor[^|]*//g; s/|align=center[^|]*//g; s/:<div[^|]*//g;' |\n",
    "    grep -v '^|-$' | grep -v '^!' | sed 's/|//g' | \n",
    "    awk 'BEGIN {printf \"Country\\tYear\\tPosition\\n\"} /^[A-Z]/ {country=$0}\n",
    "        /^[0-9]+ \\(.*\\)/ || /^ —/ {\n",
    "            if($0 != \" —\"){\n",
    "                split($0,n_year,\"\\(\"); \n",
    "                split(n_year[2], year_str, \"\\)\"); \n",
    "                len = split(year_str[1], year, \", \");\n",
    "                position = int(NR-1)%6\n",
    "                for(j=0; j<len; j++){\n",
    "                    print country\"\\t\"year[j+1]\"\\t\"position\n",
    "                }   \n",
    "            }\n",
    "            #else{ print $0 }\n",
    "        }' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Questions\n",
    "\n",
    "1. From your experience, briefly discuss the pro and cons between using Data Wrangler as compared to lower levels tools like sed/awk?\n",
    "2. What additional operations would have made using Data Wrangler \"easier\"?\n",
    "\n",
    "#### Note\n",
    "While responding to markdown cells (as the one below), in case you struggle with formatting, just double click any of the markdown cells in the notebook to see how formatting is done. You may also consult the documentation [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add your response below:\n",
    "\n",
    "1. Wrangler has a better user interface that is able to interactive with users immediately, but sometimes it works laggily. Moreover, Wrangler use it's own defined regular expressions which should be get used to them, but hopefully, more intuitively. On the other hand, sed/awk use traditional regualr expression that allow us to go into work quickly, but it is somehow quite difficult for person who does not familiar with regular expression. Besdies, sed/awk efficiently deal with all the rows of data simultaneously, but somehow quite hard to understand for a beginner.\n",
    "\n",
    "2. As mentioned above, Wrangler use it's own defined regular expressions-- such as {any}, {digit}, etc-- that allow users intuitively process the data. Also, Wrangler provides log of commands that allows users to undo or jump back to previous steps quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Tableau\n",
    "***\n",
    "\n",
    "Finally, you will perform data exploration with Tableau.\n",
    "\n",
    "1. Go to the [Tableau Website](https://www.tableau.com/), and download a demo version of Tableau.\n",
    "\n",
    "    * Tableau gives students a 1 year free license, which can be requested [here](http://www.tableau.com/freeforstudents).\n",
    "\n",
    "2. Connect Tableau to the the OnTime database, hosted on a postgreSQL server we set up for the course.\n",
    "To do so, create a new Tableau workbook.\n",
    "In the workbook, goto data and add a new data source using the credentials below:\n",
    "\n",
    "    * Type: PostgreSQL (you may have to download an additional driver for postgreSQL, [here](https://www.tableau.com/support/drivers)) (only if you can't connect)\n",
    "    * Hostname: pg-001.db.gcloud.instabase.com\n",
    "    * Username: columbia\n",
    "    * Password: B%38Mt5W@M*QU?Ar\n",
    "    * Database: db_fea10998_f88d_4b6e_8f90_a6cd73bac65c\n",
    "    * You should use the table called \"Ontime\".\n",
    "    \n",
    "3. Explore the dataset using Tableau.\n",
    "\n",
    "The aim of this assignment is to understand (1) which flights are the likeliest to be delayed (2) why they are delayed (3) what we could have missed in the data\n",
    "\n",
    "**(1) Which flights are delayed? (You're expected to answer any 2 of the 5 questions below)**\n",
    "- Long flights or short flights?\n",
    "- Which companies?\n",
    "- At what time of the day?\n",
    "- Which state are the most impacted?\n",
    "- Take California. Which cities are the most concerned? And how about NY state?\n",
    "\n",
    "**(2) Why are flights delayed? (You're expected to answer any 2  of the 6 questions below)**\n",
    "- What is the likeliest cause of delays?\n",
    "- Does that depend on the region?\n",
    "- Compare California and NY state\n",
    "- Compare Morning flights and evening flights\n",
    "- Compare weekends and rest of the week\n",
    "- Compare first week of dataset and last week\n",
    "\n",
    "**(3) what we could have missed in the data**\n",
    "- Find three quirky facts about flight delays. Anything goes, as long it involves at least one aggregate and one filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add you response below:\n",
    "\n",
    "***\n",
    "1(a) (Question): Which flights are delayed: Long flights or short flights?\n",
    "\n",
    "1(a) (Response): According to the definition in https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Flight_length.html, the short flights have length shorter than 3hr (180 mins) and long flights have length greater than 6hr (360 mins). Hence, referring to the size of the bars, we can find that there are more short-hual flights than long-hual flights. However, long-hual flights have higher frequency of delay than other kinds of flights by 6% (Medium-hual) and 10%(Short-hual)\n",
    "\n",
    "![3.1.png](https://www.dropbox.com/s/vz7gkrwa45up0q6/HW2_3.1.png?raw=1)\n",
    "\n",
    "***\n",
    "1(b) (Question): Which companies delayed the most?\n",
    "\n",
    "1(b) (Response): According to the definition in FFA (https://www.faa.gov/), if flights arrive 15 mins later is delayed; otherwise it's not. From the below graph, we can see that Carrier MQ and F9 have higher frequency of delay. That is up to 30% of their flights arrived later the the plan.\n",
    "\n",
    "![3.2.png](https://www.dropbox.com/s/q31wcobckcx4mzi/HW2_3.2.png?raw=1)\n",
    "\n",
    "***\n",
    "2(a) (Question): Why are flights delayed by comparing California and NY state?\n",
    "\n",
    "2(a) (Response): According to the size of the circle, we can observe that there are more flights flying to CA than NY. However, the delayed rate of CA is lower than the rate of NY. This refers that different states might affect different delayed rate.\n",
    "\n",
    "![3.3.png](https://www.dropbox.com/s/k72ac586z3t5dil/HW2_3.3.png?raw=1)\n",
    "\n",
    "***\n",
    "2(b) (Question): Why are flights delayed by comparing weekends and rest of the week?\n",
    "\n",
    "2(b) (Response): According to the 2nd graph below, we can understand that the total number of flights in weekdays is greater than weekends, but through looking at each ratio, we can see that the ratio of weekdays is 26.61% which is about equal to the number of weekend divide to seven days: $\\frac{2}{7} = 28.5$%. This means that on average, the total number of flights on each day in the weekends does not have huge different with the one in the weekdays. However, by the 1st graph below, weekends have slightly higher rate of delay than weekdays though there is no huge difference on the number of flights each day. Hence, the day of the flights might affect the rate of delay.\n",
    "\n",
    "![3.3.png](https://www.dropbox.com/s/uyn9xfpyvfz7iky/HW2_3.4.png?raw=1)\n",
    "\n",
    "***\n",
    "3 (Response): There are many factors can be considered while discussing flight delays. As discussed above, locations, days (time), type of flights, and carriers. By aggregating and filtering the delay, we might lose some information during the process. \n",
    "\n",
    " 1) When we group by the day of the week and filter by `Arrdelaytime`, we will easily miss the data of different carriers. Moreover, the Deptime and Arrtime seems to be incomprehensible and there are some missing data, either.\n",
    "    \n",
    " 2) While we consider the location (`Desstate`) and filter by `isDelay` (`Arrdel15`), we will miss the details about time of the flights; also, it is difficult for us to determine what the relationship is between origin and destination causing delay.\n",
    "    \n",
    " 3) As our discussion in Q1.(a), we understand that long-hual flights have higher probability to delay than short or medium-hual flights. However, there are only some specific carriers that offer long-hual flights. This refers that the carriers providing long-hual flights might have higher delayed rate than other carriers that only provide short or medium-hual flights. Thus, when we group by different carriers, we might mis-summarize that some companies delay more than the others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "* Got to 'File' and download this notebook as .ipynb\n",
    "* Rename it as **data\\_processing\\_[your uni].ipynb**\n",
    "* Then submit it on courseworks\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
